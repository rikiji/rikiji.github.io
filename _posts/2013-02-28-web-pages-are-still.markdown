---
layout: post
title: Web pages are still only meant for humans
published: false
---
The interactions between different front end web applications are strictly limited to the API offered by the communication party, often too much limited. I found myself multiple times resorting to write custom scraping scripts in order to extract the few bits of information necessary to do the job. Being able to adapt the content of a web page dynamically based on the content of other ones is something that should be given for granted in the current status of the technology, but it is often not so simple.

RSS is more misused than used, embedding ads and truncating text paragraphs after a dozen of words is the norm. It seldomly reflects the content displayed on the page, it is mostly useful as a decoy to have humans come back to the site through RSS readers. Information is precious but if it can be given away for free on a HTML page I would see no problem in offering it through a json API or in its full form on a RSS xml file. Maybe we just need better off-the-shelf parsers.
